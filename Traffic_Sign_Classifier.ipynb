{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "#Load the Images and labels\n",
    "training_file = 'data/train.p'\n",
    "validation_file= 'data/valid.p'\n",
    "testing_file = 'data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "#In memory load the mapping of label to text\n",
    "label_mapping = [\"\"]*43\n",
    "with open('signnames.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        label_mapping[int(row['ClassId'])] = row['SignName']\n",
    "        \n",
    "print(\"Data loading completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Basic Summary of the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_valid) == len(y_valid))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "n_train = len(X_train)\n",
    "\n",
    "n_test = len(X_test)\n",
    "\n",
    "n_valid = len(X_valid)\n",
    "\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "n_classes = numpy.unique(y_train).size\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Numner of validation examples = \", n_valid)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "\n",
    "print(\"The loaded table is\")\n",
    "t = PrettyTable(['Class ID', 'Sign Name'])\n",
    "for i in range(len(label_mapping)):\n",
    "    t.add_row([i,label_mapping[i]])\n",
    "    \n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Visualization of the images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "count = 10\n",
    "fig, axs = plt.subplots(count, 1, figsize=(count, count*3))\n",
    "\n",
    "for i in range(count):        \n",
    "    index = random.randint(0, len(X_train))\n",
    "    image = X_train[index]\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(label_mapping[y_train[index]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check count of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_list = y_train.tolist()\n",
    "label_count = [label_list.count(y) for y in range(n_classes)]\n",
    "\n",
    "plt.bar(range(n_classes),label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 2: Design and Test a Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre-process the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Step 1) Reduce the training data gap between classes.\n",
    "\n",
    "How to create more data for classes with low count?\n",
    "I augment by shifting and roatating images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Rotating an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "def rotate_image(image):\n",
    "    image = scipy.ndimage.interpolation.rotate(image, random.randrange(-10, 10), reshape=False)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = 10\n",
    "fig, axs = plt.subplots(count, 2, figsize=(count, count*3))\n",
    "\n",
    "for i in range(count):        \n",
    "    index = random.randint(0, len(X_train))\n",
    "    image = X_train[index]\n",
    "    axs[i][0].imshow(image)\n",
    "    axs[i][0].axis('off')\n",
    "    axs[i][0].set_title(label_mapping[y_train[index]])\n",
    "    \n",
    "    axs[i][1].imshow(rotate_image(image))\n",
    "    axs[i][1].axis('off')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Shifting an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "def shift_image(image):\n",
    "    image = scipy.ndimage.interpolation.shift(image, [random.randrange(-2, 2), random.randrange(-2, 2), 0])\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = 10\n",
    "fig, axs = plt.subplots(count, 2, figsize=(count, count*2))\n",
    "\n",
    "for i in range(count):        \n",
    "    index = random.randint(0, len(X_train))\n",
    "    image = X_train[index]\n",
    "    axs[i][0].imshow(image)\n",
    "    axs[i][0].axis('off')\n",
    "    axs[i][0].set_title(label_mapping[y_train[index]])\n",
    "    \n",
    "    axs[i][1].imshow(shift_image(image))\n",
    "    axs[i][1].axis('off')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Augment the data that occured less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "def create_variant(image):\n",
    "    if (random.choice([True, False])):\n",
    "        image = rotate_image(image)\n",
    "    else:\n",
    "        image = shift_image(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "sample_per_class = 2000\n",
    "\n",
    "for i in range(n_classes):\n",
    "    multiplier = math.floor(sample_per_class/label_count[i])\n",
    "    print(\"Class {} occuring {} times should be augmented {} fold\".format(i,label_count[i],multiplier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### I will now created augmented data and plot the new distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "augmented_image = []\n",
    "augmented_labels = []\n",
    "\n",
    "image_count = 0\n",
    "for image, label in zip(X_train,y_train):    \n",
    "    multiplier = math.floor(sample_per_class/label_count[label])\n",
    "    image_count = image_count + 1\n",
    "    augmented_image.append(image)\n",
    "    augmented_labels.append(label)\n",
    "    for i in range(multiplier):\n",
    "        augmented_image.append(create_variant(image))\n",
    "        augmented_labels.append(label)\n",
    "        \n",
    "augmented_list = (numpy.array(augmented_labels)).tolist()\n",
    "augmented_count = [augmented_list.count(y) for y in range(n_classes)]\n",
    "\n",
    "plt.bar(range(n_classes),augmented_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Step 2) Equalizing the images\n",
    "\n",
    "Converting to b/w and then equalizing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from numpy import newaxis\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_image(image):\n",
    "    image_bw = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    image_eq = cv2.equalizeHist(image_bw)\n",
    "    return image_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = 10\n",
    "fig, axs = plt.subplots(count, 2, figsize=(count, count*2))\n",
    "\n",
    "for i in range(count):        \n",
    "    index = random.randint(0, len(X_train))\n",
    "    image = X_train[index]\n",
    "    axs[i][0].imshow(image)\n",
    "    axs[i][0].axis('off')\n",
    "    axs[i][0].set_title(label_mapping[y_train[index]])\n",
    "    axs[i][1].imshow(process_image(image), cmap='gray')\n",
    "    axs[i][1].axis('off')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from numpy import newaxis\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Shuffle training data\n",
    "X_train_shuffled, y_train_shuffled = shuffle(augmented_image, augmented_labels)\n",
    "\n",
    "# preprocess\n",
    "X_train_processed = numpy.array([process_image(image) for image in X_train_shuffled])\n",
    "X_test_processed = numpy.array([process_image(image) for image in X_test])\n",
    "X_valid_processed = numpy.array([process_image(image) for image in X_valid])\n",
    "\n",
    "# reshape for conv layer\n",
    "X_train_reshaped = X_train_processed[..., newaxis]\n",
    "X_test_reshaped = X_test_processed[..., newaxis]\n",
    "X_valid_reshaped = X_valid_processed[..., newaxis]\n",
    "\n",
    "# normalize range\n",
    "X_train_normalized = X_train_reshaped - np.mean(X_train_reshaped)\n",
    "X_test_normalized = X_test_reshaped - np.mean(X_test_reshaped)\n",
    "X_valid_normalized = X_valid_reshaped - np.mean(X_valid_reshaped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x12.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 12), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(12))\n",
    "    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # Activation.\n",
    "    activation1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    # Pooling. Input = 28x28x12. Output = 14x14x12.\n",
    "    pool1 = tf.nn.max_pool(activation1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Layer 2: Convolutional. Output = 10x10x32.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5,5,12,32), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(32))\n",
    "    conv2 = tf.nn.conv2d(pool1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # Activation.\n",
    "    activation2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    # Pooling. Input = 10x10x32. Output = 5x5x32.\n",
    "    pool2 = tf.nn.max_pool(activation2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x32. Output = 800.\n",
    "    flat_data = flatten(pool2)\n",
    "#     flat_dropped_data = tf.nn.dropout(flat_data, 0.8)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 800. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(800, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(flat_data, fc1_W) + fc1_b\n",
    "    \n",
    "    # Activation.\n",
    "    activation2 = tf.nn.relu(fc1)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 200. Output = 84.\n",
    "    fc2_W = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2   = tf.matmul(activation2, fc2_W) + fc2_b\n",
    "    \n",
    "    # Activation.\n",
    "    activation3 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc3_W = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b = tf.Variable(tf.zeros(43))\n",
    "    fc3   = tf.matmul(activation3, fc3_W) + fc3_b\n",
    "    \n",
    "    return fc3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "#### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rate = 0.0008\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train_normalized[offset:end], y_train_shuffled[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        valid_accuracy = evaluate(X_valid_normalized, y_valid)\n",
    "        train_accuracy = evaluate(X_train_normalized, y_train_shuffled)\n",
    "        print(\"EPOCH {}/{} ...\".format(i+1,EPOCHS))\n",
    "        print(\"Training Accuracy = {:.3f}\".format(train_accuracy))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(valid_accuracy))\n",
    "        print(\"Batch Size = {}\".format(BATCH_SIZE))\n",
    "        print(\"Time taken = {}\".format(time.time()-start_time))\n",
    "        print(\"Learning rate = {}\".format(rate))\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\\nNow running Testing\")\n",
    "    test_accuracy = evaluate(X_test_normalized, y_test)\n",
    "    print(\"Testing Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Testing saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './lenet')\n",
    "    print(\"\\n\\nRunning accuracy on test set\")\n",
    "    test_accuracy = evaluate(X_test_normalized, y_test)\n",
    "    print(\"Testing Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# test on own images\n",
    "own_images_raw = np.array([mpimg.imread(\"own-images/\" + imageName) for imageName in os.listdir(\"own-images\")])\n",
    "# own_images = np.array([cv2.convertScaleAbs(image) for image in own_images_raw])\n",
    "\n",
    "count = len(own_images_raw)\n",
    "fig, axs = plt.subplots(count, 1, figsize=(count, count))\n",
    "\n",
    "print(\"Images picked from net are\")\n",
    "\n",
    "for i in range(count):        \n",
    "    index = i\n",
    "    image = own_images_raw[index]\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].axis('off')\n",
    "    #axs[i].set_title(label_mapping[y_train[index]])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Manualy Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = len(own_images_raw)\n",
    "fig, axs = plt.subplots(count, 1, figsize=(count, count))\n",
    "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "signType = [14,1,17,13,14]\n",
    "for i in range(count):        \n",
    "    index = i\n",
    "    image = own_images_raw[index]\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(label_mapping[signType[index]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "\n",
    "own_images = np.array([cv2.convertScaleAbs(image) for image in own_images_raw])\n",
    "own_processed = numpy.array([process_image(image) for image in own_images])\n",
    "\n",
    "# reshape for conv layer\n",
    "own_reshaped = own_processed[..., newaxis]\n",
    "\n",
    "# normalize range\n",
    "own_normalized = own_reshaped - np.mean(own_reshaped)\n",
    "\n",
    "#Softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './lenet')\n",
    "    sample_out = sess.run(logits, feed_dict={x: own_normalized})\n",
    "    top = sess.run(tf.nn.top_k(tf.constant(sample_out), k=1))\n",
    "    top5 = sess.run(tf.nn.top_k(tf.constant(sample_out), k=5))\n",
    "    \n",
    "fig, axs = plt.subplots(count, 6, figsize=(count*8, count*6))\n",
    "fig.subplots_adjust(hspace = 0.4, wspace=0.4)\n",
    "accurate_images = 0\n",
    "for i in range(count):\n",
    "    softmax_score = softmax(sample_out[i])\n",
    "    index = i \n",
    "    if(signType[index] == top5.indices[index][0] ):\n",
    "        accurate_images = accurate_images + 1\n",
    "    axs[i][0].imshow(own_images_raw[i])\n",
    "    axs[i][0].axis('off')\n",
    "    axs[i][0].set_title(label_mapping[signType[index]] + \" Labelled\",fontsize=20)\n",
    "    \n",
    "    axs[i][1].imshow(own_images_raw[i])\n",
    "    axs[i][1].axis('off')\n",
    "    proab = \"{0:.4f}\".format(softmax_score[top5.indices[index][0]])\n",
    "    axs[i][1].set_title(label_mapping[top5.indices[index][0]] +' '+ proab,fontsize=20)\n",
    "    \n",
    "    axs[i][2].imshow(own_images_raw[i])\n",
    "    axs[i][2].axis('off')\n",
    "    proab = \"{0:.4f}\".format(softmax_score[top5.indices[index][1]])\n",
    "    axs[i][2].set_title(label_mapping[top5.indices[index][1]] +' '+ proab,fontsize=20)\n",
    "    \n",
    "    axs[i][3].imshow(own_images_raw[i])\n",
    "    axs[i][3].axis('off')\n",
    "    proab = \"{0:.4f}\".format(softmax_score[top5.indices[index][2]])\n",
    "    axs[i][3].set_title(label_mapping[top5.indices[index][2]] +' '+ proab,fontsize=20)\n",
    "    \n",
    "    axs[i][4].imshow(own_images_raw[i])\n",
    "    axs[i][4].axis('off')\n",
    "    proab = \"{0:.4f}\".format(softmax_score[top5.indices[index][3]])\n",
    "    axs[i][4].set_title(label_mapping[top5.indices[index][3]] +' '+ proab,fontsize=20)\n",
    "    \n",
    "    axs[i][5].imshow(own_images_raw[i])\n",
    "    axs[i][5].axis('off')\n",
    "    proab = \"{0:.4f}\".format(softmax_score[top5.indices[index][4]])\n",
    "    axs[i][5].set_title(label_mapping[top5.indices[index][4]] +' '+ proab,fontsize=20)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Accuracy on external images \" + str(accurate_images*100/count) + \"%\")\n",
    "print(\"Images with softmax probabilities of top 5 predictions are\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
